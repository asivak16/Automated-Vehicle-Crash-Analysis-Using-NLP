{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "gxQoJcR4GNzI",
        "outputId": "cc9449d7-a55d-4a83-b770-1579e6d50293"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: pandas in /usr/local/lib/python3.10/dist-packages (2.1.4)\n",
            "Collecting stanfordnlp\n",
            "  Downloading stanfordnlp-0.2.0-py3-none-any.whl.metadata (8.6 kB)\n",
            "Requirement already satisfied: gensim in /usr/local/lib/python3.10/dist-packages (4.3.3)\n",
            "Requirement already satisfied: nltk in /usr/local/lib/python3.10/dist-packages (3.8.1)\n",
            "Requirement already satisfied: numpy in /usr/local/lib/python3.10/dist-packages (1.26.4)\n",
            "Requirement already satisfied: plotly in /usr/local/lib/python3.10/dist-packages (5.24.1)\n",
            "Requirement already satisfied: python-dateutil>=2.8.2 in /usr/local/lib/python3.10/dist-packages (from pandas) (2.8.2)\n",
            "Requirement already satisfied: pytz>=2020.1 in /usr/local/lib/python3.10/dist-packages (from pandas) (2024.2)\n",
            "Requirement already satisfied: tzdata>=2022.1 in /usr/local/lib/python3.10/dist-packages (from pandas) (2024.1)\n",
            "Requirement already satisfied: protobuf in /usr/local/lib/python3.10/dist-packages (from stanfordnlp) (3.20.3)\n",
            "Requirement already satisfied: requests in /usr/local/lib/python3.10/dist-packages (from stanfordnlp) (2.32.3)\n",
            "Requirement already satisfied: torch>=1.0.0 in /usr/local/lib/python3.10/dist-packages (from stanfordnlp) (2.4.1+cu121)\n",
            "Requirement already satisfied: tqdm in /usr/local/lib/python3.10/dist-packages (from stanfordnlp) (4.66.5)\n",
            "Requirement already satisfied: scipy<1.14.0,>=1.7.0 in /usr/local/lib/python3.10/dist-packages (from gensim) (1.13.1)\n",
            "Requirement already satisfied: smart-open>=1.8.1 in /usr/local/lib/python3.10/dist-packages (from gensim) (7.0.4)\n",
            "Requirement already satisfied: click in /usr/local/lib/python3.10/dist-packages (from nltk) (8.1.7)\n",
            "Requirement already satisfied: joblib in /usr/local/lib/python3.10/dist-packages (from nltk) (1.4.2)\n",
            "Requirement already satisfied: regex>=2021.8.3 in /usr/local/lib/python3.10/dist-packages (from nltk) (2024.9.11)\n",
            "Requirement already satisfied: tenacity>=6.2.0 in /usr/local/lib/python3.10/dist-packages (from plotly) (9.0.0)\n",
            "Requirement already satisfied: packaging in /usr/local/lib/python3.10/dist-packages (from plotly) (24.1)\n",
            "Requirement already satisfied: six>=1.5 in /usr/local/lib/python3.10/dist-packages (from python-dateutil>=2.8.2->pandas) (1.16.0)\n",
            "Requirement already satisfied: wrapt in /usr/local/lib/python3.10/dist-packages (from smart-open>=1.8.1->gensim) (1.16.0)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.10/dist-packages (from torch>=1.0.0->stanfordnlp) (3.16.1)\n",
            "Requirement already satisfied: typing-extensions>=4.8.0 in /usr/local/lib/python3.10/dist-packages (from torch>=1.0.0->stanfordnlp) (4.12.2)\n",
            "Requirement already satisfied: sympy in /usr/local/lib/python3.10/dist-packages (from torch>=1.0.0->stanfordnlp) (1.13.3)\n",
            "Requirement already satisfied: networkx in /usr/local/lib/python3.10/dist-packages (from torch>=1.0.0->stanfordnlp) (3.3)\n",
            "Requirement already satisfied: jinja2 in /usr/local/lib/python3.10/dist-packages (from torch>=1.0.0->stanfordnlp) (3.1.4)\n",
            "Requirement already satisfied: fsspec in /usr/local/lib/python3.10/dist-packages (from torch>=1.0.0->stanfordnlp) (2024.6.1)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.10/dist-packages (from requests->stanfordnlp) (3.3.2)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.10/dist-packages (from requests->stanfordnlp) (3.10)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.10/dist-packages (from requests->stanfordnlp) (2.2.3)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.10/dist-packages (from requests->stanfordnlp) (2024.8.30)\n",
            "Requirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.10/dist-packages (from jinja2->torch>=1.0.0->stanfordnlp) (2.1.5)\n",
            "Requirement already satisfied: mpmath<1.4,>=1.1.0 in /usr/local/lib/python3.10/dist-packages (from sympy->torch>=1.0.0->stanfordnlp) (1.3.0)\n",
            "Downloading stanfordnlp-0.2.0-py3-none-any.whl (158 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m158.8/158.8 kB\u001b[0m \u001b[31m6.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hInstalling collected packages: stanfordnlp\n",
            "Successfully installed stanfordnlp-0.2.0\n"
          ]
        }
      ],
      "source": [
        "!pip install pandas stanfordnlp gensim nltk numpy plotly"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "from stanfordnlp.server import CoreNLPClient\n",
        "from gensim.models import KeyedVectors\n",
        "from nltk.tokenize import word_tokenize\n",
        "import numpy as np\n",
        "from gensim.similarities import SoftCosineSimilarity, SparseTermSimilarityMatrix\n",
        "from gensim.corpora import Dictionary\n",
        "from sklearn.feature_extraction.text import TfidfVectorizer\n",
        "from sklearn.metrics.pairwise import cosine_similarity\n",
        "import plotly.express as px\n",
        "import plotly.graph_objects as go"
      ],
      "metadata": {
        "id": "Vdtd_bwjGsBT"
      },
      "execution_count": 11,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def clean_data_corrected(df):\n",
        "    cols_to_clean = ['Model', 'ADAS/ADS System Version', 'State or Local Permit', 'Operating Entity', 'City', 'State',\n",
        "                     'Roadway Type', 'Roadway Surface', 'Lighting', 'CP Pre-Crash Movement', 'SV Pre-Crash Movement', 'Narrative']\n",
        "    for col in cols_to_clean:\n",
        "        df[col] = df[col].str.strip().str.lower()\n",
        "    df['Incident Date'] = pd.to_datetime(df['Incident Date'], errors='coerce')\n",
        "    df[cols_to_clean] = df[cols_to_clean].fillna('unknown')\n",
        "    df['Model Year'] = pd.to_numeric(df['Model Year'], errors='coerce').fillna(df['Model Year'].median(skipna=True))\n",
        "    df['Posted Speed Limit (MPH)'] = pd.to_numeric(df['Posted Speed Limit (MPH)'], errors='coerce').fillna(df['Posted Speed Limit (MPH)'].median(skipna=True))\n",
        "    return df"
      ],
      "metadata": {
        "id": "1nVlxvhfGvxk"
      },
      "execution_count": 12,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def process_text_with_corenlp(text, client):\n",
        "    ann = client.annotate(text)\n",
        "    return ' '.join([token.word for sentence in ann.sentence for token in sentence.token])\n"
      ],
      "metadata": {
        "id": "trJ-kEppGxN6"
      },
      "execution_count": 4,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def process_narratives(narratives):\n",
        "    client = CoreNLPClient(annotators=['tokenize','ssplit','pos','lemma','ner', 'parse', 'depparse'], timeout=30000, memory='16G')\n",
        "    processed_texts = [process_text_with_corenlp(narrative, client) for narrative in narratives]\n",
        "    client.stop()\n",
        "    return processed_texts"
      ],
      "metadata": {
        "id": "tSfh_UtvG1Tx"
      },
      "execution_count": 5,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def load_glove_model(glove_path):\n",
        "    return KeyedVectors.load_word2vec_format(glove_path, binary=False)"
      ],
      "metadata": {
        "id": "UgbRIJ-DG6eQ"
      },
      "execution_count": 6,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def narrative_to_vec(narrative, glove_model):\n",
        "    words = word_tokenize(narrative.lower())\n",
        "    return np.mean([glove_model[word] for word in words if word in glove_model], axis=0)\n",
        "\n",
        "def setup_similarity_index(categories, glove_model):\n",
        "    dictionary = Dictionary([word_tokenize(desc.lower()) for desc in categories.values()])\n",
        "    corpus = [dictionary.doc2bow(word_tokenize(desc.lower())) for desc in categories.values()]\n",
        "    similarity_matrix = SparseTermSimilarityMatrix(glove_model, dictionary)\n",
        "    return SoftCosineSimilarity(corpus, similarity_matrix), dictionary\n",
        "\n",
        "def categorize_narratives(narrative_vectors, similarity_index, dictionary, categories):\n",
        "    categorized_results = []\n",
        "    for narrative_vec in narrative_vectors:\n",
        "        query_bow = dictionary.doc2bow(word_tokenize(narrative_vec))\n",
        "        similarities = similarity_index[query_bow]\n",
        "        categorized_results.append(list(categories.keys())[similarities[0][0]])\n",
        "    return categorized_results"
      ],
      "metadata": {
        "id": "zx4r0T7uG9ix"
      },
      "execution_count": 13,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def mainFunc(file_path):\n",
        "    data = pd.read_csv(file_path)\n",
        "\n",
        "    columns_to_extract = [\n",
        "        'Model', 'Model Year', 'ADAS/ADS System Version', 'State or Local Permit', 'Operating Entity',\n",
        "        'Incident Date', 'City', 'State', 'Roadway Type', 'Roadway Surface', 'Posted Speed Limit (MPH)',\n",
        "        'Lighting', 'CP Pre-Crash Movement', 'SV Pre-Crash Movement', 'Narrative'\n",
        "    ]\n",
        "\n",
        "    filtered_data = data[columns_to_extract]\n",
        "    filtered_data.loc[:, 'Narrative'] = filtered_data['Narrative'].fillna(\"unknown\")\n",
        "\n",
        "\n",
        "    categories_dict = {\n",
        "        \"Object/Obstacle avoidance\": \"crashes object obstacle pedestrians animals stopped vehicles tires\",\n",
        "        \"Head-on collision\": \"crashes head-on vehicles lost control\",\n",
        "    }\n",
        "    vectorizer = TfidfVectorizer()\n",
        "    category_vectors = vectorizer.fit_transform(categories_dict.values())\n",
        "    narrative_vector = vectorizer.transform(filtered_data['Narrative'])\n",
        "    similarities = cosine_similarity(narrative_vector, category_vectors)\n",
        "    filtered_data['Category'] = [list(categories_dict.keys())[index] for index in similarities.argmax(axis=1)]\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "    new_file_path = '/content/data/Categorized_Incident_Reports.csv'\n",
        "    filtered_data.to_csv(new_file_path, index=False)\n"
      ],
      "metadata": {
        "id": "mLbFdtfCHA_d"
      },
      "execution_count": 14,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def create_visualization_plotly(x, y, data, title, xlabel, ylabel):\n",
        "    if x == 'Category' and y == 'City':\n",
        "\n",
        "        size = data.groupby([x, y]).size().reset_index(name='Count')\n",
        "        fig = px.scatter(size, x=x, y=y, size='Count', title=title)\n",
        "        fig.update_layout(xaxis_title=xlabel, yaxis_title=ylabel)\n",
        "    elif x == 'Category' and y == 'Lighting':\n",
        "\n",
        "        count_data = data.groupby(x)[y].value_counts().unstack(fill_value=0)\n",
        "        fig = px.bar(count_data, barmode='stack', title=title)\n",
        "        fig.update_layout(xaxis_title=xlabel, yaxis_title=ylabel)\n",
        "    elif x == 'Posted Speed Limit (MPH)' and y == 'Roadway Surface':\n",
        "        pivot_table = data.pivot_table(index=x, columns=y, aggfunc='size', fill_value=0)\n",
        "        fig = go.Figure(data=go.Heatmap(\n",
        "            z=pivot_table.values,\n",
        "            x=pivot_table.columns,\n",
        "            y=pivot_table.index,\n",
        "            colorscale='YlGnBu'\n",
        "        ))\n",
        "        fig.update_layout(title=title, xaxis_title=xlabel, yaxis_title=ylabel)\n",
        "    elif x == 'State' and y == 'Category':\n",
        "\n",
        "        count_data = data.groupby(x)[y].value_counts().unstack(fill_value=0)\n",
        "        fig = px.bar(count_data, barmode='stack', title=title)\n",
        "        fig.update_layout(xaxis_title=xlabel, yaxis_title=ylabel)\n",
        "    elif x == 'State' and y == 'Operating Entity':\n",
        "\n",
        "\n",
        "\n",
        "        fig = px.histogram(data, x=x, color=y, title=title)\n",
        "        fig.update_layout(xaxis_title=xlabel, yaxis_title=ylabel)\n",
        "    elif x == 'Category' and y == 'Roadway Type':\n",
        "\n",
        "        fig = px.histogram(data, x=x, color=y, title=title)\n",
        "        fig.update_layout(xaxis_title=xlabel, yaxis_title=ylabel)\n",
        "\n",
        "    fig.show()\n"
      ],
      "metadata": {
        "id": "UUrQ47EjHI1a"
      },
      "execution_count": 15,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "file_path = '/content/data/SGO-2021-01_Incident_Reports_ADS.csv'\n",
        "data = pd.read_csv(file_path)\n",
        "cleaned_data = clean_data_corrected(data)\n",
        "\n",
        "\n",
        "cleaned_file_path = '/content/data/Cleaned_ADS_Incident_Reports.csv'\n",
        "cleaned_data.to_csv(cleaned_file_path, index=False)\n",
        "\n",
        "\n",
        "mainFunc(cleaned_file_path)\n",
        "\n",
        "\n",
        "data = pd.read_csv('/content/data/Categorized_Incident_Reports.csv')\n",
        "\n",
        "fig = px.bar(data, x='Category', title='Frequency of Incident Categories')\n",
        "fig.update_layout(xaxis_title='Category', yaxis_title='Count', xaxis={'categoryorder': 'total descending'})\n",
        "\n",
        "fig.write_html(\"incident_categories_bar_chart.html\")\n",
        "\n",
        "\n",
        "roadway_counts = data['Roadway Surface'].value_counts()\n",
        "fig = px.pie(values=roadway_counts, names=roadway_counts.index, title='Distribution of Roadway Surfaces')\n",
        "\n",
        "fig.write_html(\"roadway_surfaces_pie_chart.html\")\n",
        "\n",
        "\n",
        "fig = px.histogram(data, x='Posted Speed Limit (MPH)', nbins=15, title='Histogram of Posted Speed Limits')\n",
        "fig.update_layout(xaxis_title='Speed Limit (MPH)', yaxis_title='Frequency')\n",
        "fig.write_html(\"speed_limits_histogram.html\")\n",
        "fig = px.box(data, x='Category', y='Model Year', title='Model Year Distribution by Category')\n",
        "fig.update_layout(xaxis_title='Category', yaxis_title='Model Year')\n",
        "\n",
        "fig.write_html(\"model_year_boxplot.html\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "rpnGVzcQHMyf",
        "outputId": "1b3dc306-23cc-4ab5-b4bb-20dd608908fd"
      },
      "execution_count": 16,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "<ipython-input-12-ea9a68a314c5>:6: UserWarning: Could not infer format, so each element will be parsed individually, falling back to `dateutil`. To ensure parsing is consistent and as-expected, please specify a format.\n",
            "  df['Incident Date'] = pd.to_datetime(df['Incident Date'], errors='coerce')\n",
            "<ipython-input-14-5ff83731c99b>:27: SettingWithCopyWarning: \n",
            "A value is trying to be set on a copy of a slice from a DataFrame.\n",
            "Try using .loc[row_indexer,col_indexer] = value instead\n",
            "\n",
            "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
            "  filtered_data['Category'] = [list(categories_dict.keys())[index] for index in similarities.argmax(axis=1)]\n"
          ]
        }
      ]
    }
  ]
}